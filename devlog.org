* design

** frontend (packages/app)
- http://localhost:7891
- proxies ~/api~ and ~/sync~ to the backend in development
- uses Dexie for local storage with sync plugin
- custom sync replication implementation using PeerJS through the signalling server

** backend (packages/server)
- http://localhost:7890
- serves ~/dist~ if the directory is present (see ~dist~ script)
- serves ~/api~ for RSS caching proxy
  - file-based routing under the api directory
- serves ~/sync~ which is a ~peerjs~ signalling server

** sync
- each client keeps the full data set
- dexie sync and observable let us stream change sets
- we can publish the "latest" to all peers
- on first pull, if not the first client, we can request a dump out of band

*** rss feed data
- do we want to backup feed data?
  - conceptually, this should be refetchable
  - but feeds go away, and some will only show recent stories
    - so yes, we'll need this
  - but server side, we can dedupe
    - content-addressed server-side cache?

- server side does RSS pulling
  - can feeds be marked private, such that they won't be pulled through the proxy?
  - but then we require everything to be fetchable via cors
    - client configured proxy settings?

*** peer connection
- on startup, check for current realm-id and key pair
- if not present, ask to login or start new
  - if login, run through the [[* pairing]] process
  - if start new, run through the [[* registration]] process
- use keypair to authenticate to server
  - response includes list of active peers to connect
- clients negotiate sync from there
- an identity is a keypair and a realm

- realm is uuid
  - realm on the server is the socket connection for peer discovery
    - keeps a list of verified public keys
    - and manages the /current/ ~public-key->peer ids~ mapping
  - realm on the client side is first piece of info required for sync
    - when connecting to the signalling server, you present a realm, and a signed public key
    - server accepts/rejects based on signature and current verified keys

- a new keypair can create a realm

- a new keypair can double sign an invitation
  - invite = ~{ realm:, nonce:, not_before:, not_after:, authorizer: }~, signed with verified key
  - exchanging an invite = ~{ invite: }~, signed with my key

- on startup
  - start stand-alone (no syncing required, usually the case on first-run)
    - generate a keypair
    - want server backup?
      - sign a "setup" message with new keypair and send to the server
      - server responds with a new realm, that this keypair is already verified for
    - move along
  - exchange invite to sync to other devices
    - generate a keypair
    - sign the exchange message with the invite and send to the server
      - server verifies the invite
      - adds the new public key to the peer list and publishes downstream
    - move along

**** standalone
in this mode, there is no syncing. this is the most likely first-time run option.

- generate a keypair on startup, so we have a stable fingerprint in the future
- done

**** pairing
in this mode, there is syncing to a named realm, but not necessarily server resources consumed
we don't need an email, since the server is just doing signalling and peer management

- generate an invite from an existing verified peer
  - ~{ realm:, not_before:, not_after:, inviter: peer.public_key }~
  - sign that invitation from the existing verified peer

- standalone -> paired
  - get the invitation somehow (QR code?)
  - sign an invite exchange with the standalone's public key
  - send to server
    - server verifies the invite
    - adds the new public key to the peer list and publishes downstream

**** server backup
in this mode, there is syncing to a named realm by email.

goal of server backup mode is that we can go from email->fully working client with latest data without having to have any clients left around that could participate in the sync.

- generate a keypair on startup
- sign a registration message sent to the server
  - send a verification email
    - if email/realm already exists, this is authorization
    - if not, it's email validation
  - server starts a realm and associates the public key
  - server acts as a peer for the realm, and stores private data

- since dexie is publishing change sets, we should be able to just store deltas
- but we'll need to store _all_ deltas, unless we're materializing on the server side too
  - should we use an indexdb shim so we can import/export from the server for clean start?
  - how much materialization does the server need?

* ai instructions
- when writing to the devlog, add tags to your entries specifying ~:ai:~ and what tool did it.
- false starts and prototypes are in ~./devlog/~

* journal
** 2025
*** May 28
getting everything setup

the biggest open question I have is what sort of privacy/encryption guarantee I need. I want the server to be able to do things like cache and store feed data long-term.

Is "if you want full privacy, self-host" valid?

**** possibilities

- fully PWA
  - CON: cors, which would require a proxy anyway
  - CON: audio analysis, llm based stuff for categorization, etc. won't work
  - PRO: private as all get out
    - can still do WebRTC p2p sync for resiliancy
    - can still do server backups, if sync stream is encrypted, but no compaction would be available
    - could do _explicit_ server backups as dump files

- self hostable
  - PRO: can do bunches of private stuff on the server, because if you don't want me to see it, do it elsewhere
  - CON: hard for folk to use

**** sync conflict resolution design discussion                   :ai:claude:

discussed the sync architecture and dexie conflict handling:

**dexie syncable limitations**:
- logical clocks handle causally-related changes well
- basic timestamp-based conflict resolution for concurrent updates
- last-writer-wins for same field conflicts
- no sophisticated CRDT or vector clock support

**solutions for podcast-specific conflicts**:

- play records: device-specific approach
  - store separate play_records per device_id
  - each record: { episode_id, device_id, position, completed, timestamp }
  - UI handles conflict resolution with "continue from X device?" prompts
  - avoids arbitrary timestamp wins, gives users control

- subscription trees: parent_path field
  - store parent_path as single string field ("/Tech/Programming")
  - simpler than managing folder membership tables
  - conflicts still possible but contained to single field
  - could store move operations as events for richer resolution

**other sync considerations**:
- settings/preferences: distinguish device-local vs global
- bulk operations: "mark all played" can create duplicate operations
- metadata updates: server RSS updates vs local renames
- temporal ordering: recently played lists, queue reordering
- storage limits: cleanup operations conflicting across devices
- feed state: refresh timestamps, error states

**approach**: prefer "events not state" pattern and device-specific records where semantic conflicts are likely

**** data model brainstorm                                        :ai:claude:

core entities designed with sync in mind:

**Feed** - RSS/podcast subscription
- parent_path field for folder structure ("/Tech/Programming")
- is_private flag to skip server proxy
- refresh_interval for custom update frequencies

**Episode** - individual podcast episodes
- standard RSS metadata (guid, title, description, media url)
- duration and file info for playback

**PlayRecord** - device-specific playback state
- separate record per device_id to avoid timestamp conflicts
- position, completed status, playback speed
- UI can prompt "continue from X device?" for resolution

**QueueItem** - device-specific episode queue
- ordered list with position field
- device_id scoped to avoid queue conflicts

**Subscription** - feed membership settings
- can be global or device-specific
- auto_download preferences per device

**Settings** - split global vs device-local
- theme, default speed = global
- download path, audio device = device-local

**Event tables** for complex operations:
- FeedMoveEvent for folder reorganization
- BulkMarkPlayedEvent for "mark all read" operations
- better conflict resolution than direct state updates

**sync considerations**:
- device identity established on first run
- dexie syncable handles basic timestamp conflicts
- prefer device-scoped records for semantic conflicts
- event-driven pattern for bulk operations

**** schema evolution from previous iteration                     :ai:claude:

reviewed existing schema from tmp/feed.ts - well designed foundation:

**keep from original**:
- Channel/ChannelEntry naming and structure
- refreshHP adaptive refresh system (much better than simple intervals)
- rich podcast metadata (people, tags, enclosure, podcast object)
- HTTP caching with etag/status tracking
- epoch millisecond timestamps
- hashId() approach for entry IDs

**add for multi-device sync**:
- PlayState table (device-scoped position/completion)
- Subscription table (with parentPath for folders, device-scoped settings)
- QueueItem table (device-scoped episode queues)
- Device table (identity management)

**migration considerations**:
- existing Channel/ChannelEntry can be preserved
- new tables are additive
- fetchAndUpsert method works well with server proxy architecture
- dexie sync vs rxdb - need to evaluate change tracking capabilities

**** content-addressed caching for offline resilience              :ai:claude:

designed caching system for when upstream feeds fail/disappear, building on existing cache-schema.ts:

**server-side schema evolution** (drizzle sqlite):
- keep existing httpCacheTable design (health tracking, http headers, ttl)
- add contentHash field pointing to deduplicated content
- new contentStoreTable: deduplicated blobs by sha256 hash
- new contentHistoryTable: url -> contentHash timeline with isLatest flag
- reference counting for garbage collection

**client-side OPFS storage**:
- /cache/content/{contentHash}.xml for raw feeds
- /cache/media/{contentHash}.mp3 for podcast episodes
- LocalCacheEntry metadata tracks expiration and offline-only flags
- maintains last N versions per feed for historical access

**fetch strategy & fallback**:
1. check local OPFS cache first (fastest)
2. try server proxy /api/feed?url={feedUrl} (deduplicated)
3. server checks contentHistory, serves latest or fetches upstream
4. server returns {contentHash, content, cached: boolean}
5. client stores with content hash as filename
6. emergency mode: serve stale content when upstream fails

**benefits**:
- preserves existing health tracking and HTTP caching logic
- popular feeds cached once on server, many clients benefit
- bandwidth savings via content hash comparison
- historical feed state preservation (feeds disappear!)
- true offline operation after initial sync
- deduplication across time and users

**** proposed schemas                                              :ai:claude:

**client-side schema (dexie)**:

#+BEGIN_SRC typescript
// Core RSS/Podcast data (from your existing design)
interface Channel {
  id: string;
  feedUrl: string;
  htmlUrl?: string;
  imageUrl?: string;
  title?: string;
  description?: string;
  language?: string;
  people?: Record<string, string>;
  tags?: string[];

  // Refresh management
  refreshHP: number;
  nextRefreshAt?: number;
  lastRefreshAt?: number;
  lastRefreshStatus?: string;
  lastRefreshHttpStatus?: number;
  lastRefreshHttpEtag?: string;

  // Cache info
  contentHash?: string;
  lastFetchedAt?: number;
}

interface ChannelEntry {
  id: string;
  channelId: string;
  guid: string;
  title: string;
  linkUrl?: string;
  imageUrl?: string;
  snippet?: string;
  content?: string;

  enclosure?: {
    url: string;
    type?: string;
    length?: number;
  };

  podcast?: {
    explicit?: boolean;
    duration?: string;
    seasonNum?: number;
    episodeNum?: number;
    transcriptUrl?: string;
  };

  publishedAt?: number;
  fetchedAt?: number;
}

// Device-specific sync tables
interface PlayRecord {
  id: string;
  entryId: string;
  deviceId: string;
  position: number;
  duration?: number;
  completed: boolean;
  speed: number;
  updatedAt: number;
}

interface Subscription {
  id: string;
  channelId: string;
  deviceId?: string;
  parentPath: string;  // "/Tech/Programming"
  autoDownload: boolean;
  downloadLimit?: number;
  isActive: boolean;
  createdAt: number;
  updatedAt: number;
}

interface QueueItem {
  id: string;
  entryId: string;
  deviceId: string;
  position: number;
  addedAt: number;
}

interface Device {
  id: string;
  name: string;
  platform: string;
  lastSeen: number;
}

// Local cache metadata
interface LocalCache {
  id: string;
  url: string;
  contentHash: string;
  filePath: string;    // OPFS path
  cachedAt: number;
  expiresAt?: number;
  size: number;
  isOfflineOnly: boolean;
}

// Dexie schema
const db = new Dexie('SkypodDB');
db.version(1).stores({
  channels: '&id, feedUrl, contentHash',
  channelEntries: '&id, channelId, publishedAt',
  playRecords: '&id, [entryId+deviceId], deviceId, updatedAt',
  subscriptions: '&id, channelId, deviceId, parentPath',
  queueItems: '&id, entryId, deviceId, position',
  devices: '&id, lastSeen',
  localCache: '&id, url, contentHash, expiresAt'
});
#+END_SRC

**server-side schema**:

#+BEGIN_SRC typescript
// Content-addressed cache
interface ContentStore {
  contentHash: string;     // Primary key
  content: Buffer;         // Raw feed content
  contentType: string;
  contentLength: number;
  firstSeenAt: number;
  referenceCount: number;
}

interface ContentHistory {
  id: string;
  url: string;
  contentHash: string;
  fetchedAt: number;
  isLatest: boolean;
}

// HTTP cache with health tracking (from your existing design)
interface HttpCache {
  key: string;             // URL hash, primary key
  url: string;

  status: 'alive' | 'dead';
  lastFetchedAt: number;
  lastFetchError?: string;
  lastFetchErrorStreak: number;

  lastHttpStatus: number;
  lastHttpEtag?: string;
  lastHttpHeaders: Record<string, string>;
  expiresAt: number;
  expirationTtl: number;

  contentHash: string;     // Points to ContentStore
}

// Sync/auth tables
interface Realm {
  id: string;              // UUID
  createdAt: number;
  verifiedKeys: string[];  // Public key list
}

interface PeerConnection {
  id: string;
  realmId: string;
  publicKey: string;
  lastSeen: number;
  isOnline: boolean;
}

// Media cache for podcast episodes
interface MediaCache {
  contentHash: string;     // Primary key
  originalUrl: string;
  mimeType: string;
  fileSize: number;
  content: Buffer;
  cachedAt: number;
  accessCount: number;
}
#+END_SRC
